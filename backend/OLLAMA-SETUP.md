# ğŸ¤– ConfiguraciÃ³n de Ollama para Reportes de Surf

Este documento explica cÃ³mo configurar y usar Ollama como proveedor de IA local para generar reportes de surf.

## ğŸ“‹ Requisitos Previos

- Ollama instalado en tu sistema
- Al menos 8GB de RAM disponible
- Espacio en disco para el modelo (~4GB para qwen2.5:7b)

## ğŸš€ InstalaciÃ³n de Ollama

### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### macOS
```bash
brew install ollama
```

### Windows
Descarga el instalador desde: https://ollama.com/download

## ğŸ“¦ Descargar el Modelo

Descarga el modelo Qwen 2.5 (7B parÃ¡metros):

```bash
ollama pull qwen2.5:7b
```

**Modelos alternativos recomendados:**
```bash
# MÃ¡s rÃ¡pido pero menos preciso (3B)
ollama pull qwen2.5:3b

# MÃ¡s preciso pero mÃ¡s lento (14B)
ollama pull qwen2.5:14b

# Modelo general alternativo
ollama pull llama3.1:8b
```

## âš™ï¸ ConfiguraciÃ³n

### 1. Iniciar Ollama

```bash
# Inicia el servidor de Ollama
ollama serve
```

Por defecto corre en `http://localhost:11434`

### 2. Configurar Variables de Entorno

Agrega a tu archivo `.env`:

```env
# Proveedor de IA (gemini u ollama)
AI_PROVIDER=ollama

# ConfiguraciÃ³n de Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b
```

### 3. Verificar InstalaciÃ³n

Ejecuta el script de prueba:

```bash
npm run test:ollama
```

DeberÃ­as ver:
```
âœ… Ollama estÃ¡ corriendo
âœ… Modelo qwen2.5:7b estÃ¡ disponible
ğŸ“ REPORTE GENERADO: ...
```

## ğŸ¯ Uso

### OpciÃ³n 1: Configurar como Proveedor por Defecto

En `.env`:
```env
AI_PROVIDER=ollama
```

Luego genera reportes normalmente:
```bash
POST /api/reports/generate
{
  "spotId": "mdp_playa_grande"
}
```

### OpciÃ³n 2: Especificar en cada Request

```bash
POST /api/reports/generate
{
  "spotId": "mdp_playa_grande",
  "aiProvider": "ollama"
}
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Cambiar Modelo en Runtime

```typescript
import { OllamaProvider } from './strategy/OllamaProvider';

const provider = new OllamaProvider({
  apiKey: '', // No necesario para Ollama
  baseUrl: 'http://localhost:11434',
  model: 'llama3.1:8b' // Usa modelo alternativo
});
```

### Ajustar ParÃ¡metros del Modelo

En `OllamaProvider.ts`, modifica las opciones:

```typescript
options: {
  temperature: 0.7,  // 0.0 = determinÃ­stico, 1.0 = creativo
  top_p: 0.9,        // Muestreo nucleus
  top_k: 40,         // Top-k sampling
}
```

### Usar Ollama Remoto

Si Ollama estÃ¡ en otro servidor:

```env
OLLAMA_BASE_URL=http://192.168.1.100:11434
```

## ğŸ“Š ComparaciÃ³n de Modelos

| Modelo | TamaÃ±o | RAM | Velocidad | Calidad | Recomendado para |
|--------|--------|-----|-----------|---------|------------------|
| qwen2.5:3b | ~2GB | 4GB | âš¡âš¡âš¡ | â­â­ | Pruebas rÃ¡pidas |
| qwen2.5:7b | ~4GB | 8GB | âš¡âš¡ | â­â­â­ | **ProducciÃ³n** |
| qwen2.5:14b | ~8GB | 16GB | âš¡ | â­â­â­â­ | Alta calidad |
| llama3.1:8b | ~5GB | 8GB | âš¡âš¡ | â­â­â­ | Alternativa |

## ğŸ› Troubleshooting

### Error: "ECONNREFUSED"
```
âŒ No se pudo conectar con Ollama
```

**SoluciÃ³n:**
```bash
# Verifica que Ollama estÃ© corriendo
ollama serve

# O en background
nohup ollama serve > /dev/null 2>&1 &
```

### Error: "Model not found"
```
âŒ Modelo qwen2.5:7b no encontrado
```

**SoluciÃ³n:**
```bash
ollama pull qwen2.5:7b
```

### Listar Modelos Instalados
```bash
ollama list
```

### Eliminar un Modelo
```bash
ollama rm qwen2.5:7b
```

### Ver Logs de Ollama
```bash
# Linux/Mac
journalctl -u ollama -f

# O en la consola donde ejecutaste ollama serve
```

## ğŸ”„ ComparaciÃ³n: Ollama vs Gemini

| CaracterÃ­stica | Ollama | Gemini |
|----------------|---------|--------|
| **Costo** | âœ… Gratis | âš ï¸ Pago por uso |
| **Privacidad** | âœ… 100% local | âŒ Cloud |
| **Velocidad** | âš¡ Media (depende de HW) | âš¡âš¡ RÃ¡pido |
| **Calidad** | â­â­â­ Buena | â­â­â­â­ Excelente |
| **Requisitos** | ğŸ–¥ï¸ 8GB RAM | ğŸŒ Internet + API Key |
| **Disponibilidad** | âœ… Offline | âŒ Requiere conexiÃ³n |

## ğŸ’¡ Recomendaciones

1. **Para Desarrollo**: Usa Ollama (gratis, offline)
2. **Para ProducciÃ³n**: Gemini (mejor calidad) o Ollama con modelo grande
3. **Sin Internet**: Solo Ollama funciona
4. **Hardware Limitado**: Usa `qwen2.5:3b` o Gemini
5. **MÃ¡xima Calidad**: Usa `qwen2.5:14b` o Gemini

## ğŸ“š Recursos

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Qwen Models](https://ollama.com/library/qwen2.5)
- [Model Library](https://ollama.com/library)

## ğŸ¨ Ejemplo Completo

```typescript
// backend/src/scripts/testOllama.ts
import { OllamaProvider } from './strategy/OllamaProvider';

const provider = new OllamaProvider({
  apiKey: '',
  baseUrl: 'http://localhost:11434',
  model: 'qwen2.5:7b'
});

// Verificar conexiÃ³n
const isReady = await provider.testConnection();

// Generar reporte
const report = await provider.generateReport({
  weatherData: {...},
  localidadNombre: 'Playa Grande',
  spot: {...}
});

console.log(report);
```

---

**Â¿Necesitas ayuda?** Abre un issue o consulta la documentaciÃ³n de Ollama.
